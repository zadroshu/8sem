{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Categorical\n",
        "import numpy as np\n",
        "import time\n",
        "import random"
      ],
      "metadata": {
        "id": "YusOtuNEptRr"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Игра\n",
        "class Kalah:\n",
        "    def __init__(self, num_holes=6, num_seeds=6.):\n",
        "        self.num_holes = num_holes\n",
        "        self.num_seeds = num_seeds\n",
        "\n",
        "        self.board = [float(num_seeds) for _ in range(num_holes * 2 + 2)]  # Заполняем лунки\n",
        "\n",
        "        self.kalah1_index = self.num_holes  # Индекс корзины первого игрока\n",
        "        self.kalah2_index = self.num_holes + 1  # Индекс корзины второго игрока\n",
        "\n",
        "        self.board[self.kalah1_index] = 0.\n",
        "        self.board[self.kalah2_index] = 0.\n",
        "\n",
        "        self.current_player = 0  # Текущий игрок\n",
        "\n",
        "        self.diff1 = self.num_holes + 2  # Разница между противоположными лунками относительно прямого порядка\n",
        "        self.diff2 = self.num_holes  # Разница между противоположными лунками относительно обратного порядка\n",
        "\n",
        "    def step(self, action):\n",
        "        hole = action\n",
        "        player = self.current_player\n",
        "        boardCopy = self.board.copy()\n",
        "        if player == 1: hole -= self.num_holes\n",
        "\n",
        "        # Если в выбранной лунке 0 камней, заканчиваем игру и отдаем все камни противоположному игроку\n",
        "        if boardCopy[hole] == 0:\n",
        "            for i in range(len(boardCopy)):\n",
        "                boardCopy[i] = 0.\n",
        "            boardCopy[self.num_holes + 1 - player] = 2 * self.num_holes * self.num_seeds\n",
        "            return [boardCopy, player]\n",
        "        \n",
        "        seeds = boardCopy[hole]  # Запоминаем количество в выбранной лунке\n",
        "\n",
        "        boardCopy[hole] = 0.  # Обнуляем выбранную лунку\n",
        "\n",
        "        # Запускаем распределение камней\n",
        "        while seeds > 0:\n",
        "            hole = hole + 1 if hole >= 0 else hole - 1\n",
        "\n",
        "            # Если прошли свою корзину, переходим на другую сторону\n",
        "            if hole == self.num_holes + 1:\n",
        "                hole = -1\n",
        "            if hole == -1 * self.num_holes - 2:\n",
        "                hole = 0\n",
        "\n",
        "            # Если распределение дошло до корзины соперника, пропускаем её\n",
        "            if hole == self.num_holes and player == 1:\n",
        "                continue\n",
        "            if hole == -1 * self.num_holes - 1 and player == 0:\n",
        "                continue\n",
        "\n",
        "            boardCopy[hole] += 1.  # Увеличиваем кол-во камней в лунке\n",
        "            seeds -= 1.  # Уменьшаем кол-во камней в выбранной лунке\n",
        "\n",
        "        # Если последний камень оказался в корзине, выходим без смены хода\n",
        "        if hole == self.num_holes:\n",
        "            return [boardCopy, player]\n",
        "        if hole == -1 * self.num_holes - 1:\n",
        "            return [boardCopy, player]\n",
        "\n",
        "        # Если последний камень попал в пустую лунку принадлежащую ему и противоположная лунка соперника не пуста, то этот камень и все камни из противоположной лунки соперника игрок переносит себе в корзину\n",
        "        if boardCopy[hole] == 1 and boardCopy[hole + self.diff1 if player == 0 else hole + self.diff2] > 0:\n",
        "            if hole >= 0 and player == 0 or hole < 0 and player == 1:\n",
        "                boardCopy[self.num_holes if hole >= 0 else self.num_holes + 1] += boardCopy[hole + self.diff1 if hole >= 0 else hole + self.diff2] + 1.\n",
        "                boardCopy[hole] = 0.\n",
        "                boardCopy[hole + self.diff1 if player == 0 else hole + self.diff2] = 0.\n",
        "\n",
        "        player = 1 - player\n",
        "        return [boardCopy, player]\n",
        "    \n",
        "    def do_step(self, action):\n",
        "        tmp = self.step(action)\n",
        "        self.board = tmp[0]\n",
        "        self.current_player = tmp[1]\n",
        "\n",
        "    def game_over(self):\n",
        "        return sum(self.board[:self.num_holes]) == 0 or sum(self.board[self.diff1:]) == 0 or self.board[\n",
        "            -1 * self.num_holes - 1] > (self.num_holes * self.num_holes) or self.board[self.num_holes] > (\n",
        "                           self.num_holes * self.num_holes)\n",
        "\n",
        "    def get_winner(self):\n",
        "        if not self.game_over():\n",
        "            return None\n",
        "        return 0 if self.board[self.num_holes] > self.board[-1 * self.num_holes - 1] else 1\n",
        "\n",
        "    def get_value(self, action):\n",
        "        hole = action\n",
        "        if self.current_player == 1: hole -= self.num_holes\n",
        "        return self.board[hole]\n",
        "\n",
        "    def get_state(self):\n",
        "        return [self.board, self.current_player]\n",
        "    \n",
        "    def get_valid_moves(self):\n",
        "        moves = []\n",
        "        for i in range(self.num_holes):\n",
        "            if self.get_value(i) != 0:\n",
        "                moves.append(i)\n",
        "        return moves\n",
        "\n",
        "    def play_game(self):\n",
        "        move = 0\n",
        "        while not self.game_over():\n",
        "            print(\"=========== Move \", move, \"===========\")\n",
        "            print(\"player 2:\", self.board[-1 * self.num_holes - 1], self.board[self.num_holes + 2:])\n",
        "            print(\"player 1:  \", self.board[:self.num_holes], self.board[self.num_holes])\n",
        "            hole = int(input(\"Player {}'s turn. Enter hole number(0-5): \".format(self.current_player + 1)))\n",
        "            while True:\n",
        "                if hole >= 0 and hole <= 5: break\n",
        "                hole = int(input(\"Player {}'s turn. Enter hole number(0-5): \".format(self.current_player + 1)))\n",
        "            self.do_step(hole)\n",
        "            move += 1\n",
        "        print(\"Game over. Player {} wins!\".format(self.get_winner() + 1))\n",
        "        print(\"player 2:\", self.board[-1 * self.num_holes - 1], self.board[self.num_holes + 2:])\n",
        "        print(\"player 1:  \", self.board[:self.num_holes], self.board[self.num_holes])\n",
        "\n",
        "    def playBolvanVsBolvan(self):\n",
        "        move = 0\n",
        "        while not self.game_over():\n",
        "            print(\"=========== Move \", move, \"===========\")\n",
        "            print(\"player 2:\", self.board[-1 * self.num_holes - 1], self.board[self.num_holes + 2:])\n",
        "            print(\"player 1:  \", self.board[:self.num_holes], self.board[self.num_holes])\n",
        "            hole = bolvan(self.board, self.current_player)\n",
        "            print(\"bolvan{} action {}; value {}\\n\".format(self.current_player+1, hole, self.get_value(hole)))\n",
        "            self.do_step(hole)\n",
        "            move += 1\n",
        "        print(\"Game over. Player {} wins!\".format(self.get_winner() + 1))\n",
        "        print(\"player 2:\", self.board[-1 * self.num_holes - 1], self.board[self.num_holes + 2:])\n",
        "        print(\"player 1:  \", self.board[:self.num_holes], self.board[self.num_holes])"
      ],
      "metadata": {
        "id": "3_0LEBFKkLS8"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Функция для инициализации модели\n",
        "file_name = 'kallah_neyron'\n",
        "\n",
        "def model_init():\n",
        "    DL = 14\n",
        "    Prs = 14\n",
        "\n",
        "    model = 0\n",
        "\n",
        "    if os.path.exists(file_name):\n",
        "        model = torch.load(file_name)\n",
        "        print(\"model loaded\")\n",
        "    # else:\n",
        "    #     model = nn.Sequential(\n",
        "    #         nn.Linear(DL,Prs),\n",
        "    #         nn.Sigmoid(),\n",
        "    #         nn.Linear(Prs, 6),\n",
        "    #         nn.Softmax()\n",
        "    #     )\n",
        "    #     print(\"model init\")\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "TQcE2UqElkGi"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Болван\n",
        "def bolvan(game):\n",
        "    return random.choice(game.get_valid_moves())"
      ],
      "metadata": {
        "id": "Q__XvD3GlmYb"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def space(board):\n",
        "    return torch.tensor(board[0:])"
      ],
      "metadata": {
        "id": "nz84UOqNlnsv"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Функция тренировки\n",
        "def train(gamma = 0.9, alpha = 0.45, max_ep = 99999999):\n",
        "    time_1 = time.time()\n",
        "    zero_flag = False\n",
        "    count_errors = 0\n",
        "    wins_neural = 0\n",
        "    model = model_init()\n",
        "    optimizer = torch.optim.Adam(model.parameters())\n",
        "    move = 0\n",
        "    reward_for_zero = -500\n",
        "\n",
        "    for episode in range(max_ep):\n",
        "        if episode % 1000 == 0:\n",
        "            print(\"Neural wins per 1000 plays {}: errors per 1000 plays {}\".format(wins_neural, count_errors))\n",
        "            count_errors = 0\n",
        "            wins_neural = 0\n",
        "\n",
        "            torch.save(model, file_name)\n",
        "        \n",
        "        tr = []\n",
        "        game = Kalah()\n",
        "        move = 0\n",
        "        while not game.game_over():\n",
        "            state = game.get_state()\n",
        "            cur_board = state[0]\n",
        "            cur_player = state[1]\n",
        "            SPACE = space(cur_board)\n",
        "\n",
        "            if cur_player == 0:\n",
        "                probs = model(SPACE)\n",
        "                m = Categorical(probs)\n",
        "                action = m.sample()\n",
        "            else:\n",
        "                action = bolvan(game)\n",
        "\n",
        "            # print(\"\\n=========== Ep {}; Move {}; Player {}: action {}: ===========\".format(episode, move, cur_player+1, action))\n",
        "            # print(\"player 2:\", game.board[-1 * game.num_holes - 1], game.board[game.num_holes + 2:])\n",
        "            # print(\"player 1:  \", game.board[:game.num_holes], game.board[game.num_holes])\n",
        "\n",
        "            # if(game.get_value(action) == 0):\n",
        "            #     print(\"=========== Ep {}; Move {}; Player {}: action {}: ===========\".format(episode, move, cur_player+1, action))\n",
        "            #     print(\"player 2:\", game.board[-1 * game.num_holes - 1], game.board[game.num_holes + 2:])\n",
        "            #     print(\"player 1:  \", game.board[:game.num_holes], game.board[game.num_holes])\n",
        "\n",
        "            if cur_player == 0 and game.get_value(action) != 0:\n",
        "                reward = 1\n",
        "                #reward = game.board[game.num_holes] - game.board[-1 * game.num_holes - 1]\n",
        "                tr.append((SPACE.clone(), action.clone(), reward))\n",
        "            elif cur_player == 0 and game.get_value(action) == 0:\n",
        "                reward = reward_for_zero\n",
        "                tr.append((SPACE.clone(), action.clone(), reward))\n",
        "                count_errors += 1\n",
        "\n",
        "            game.do_step(action)\n",
        "            move += 1\n",
        "        if game.get_winner() == 0:\n",
        "          wins_neural += 1 \n",
        "\n",
        "        # print(\"\\nplayer 2:\", game.board[-1 * game.num_holes - 1], game.board[game.num_holes + 2:])\n",
        "        # print(\"player 1:  \", game.board[:game.num_holes], game.board[game.num_holes])\n",
        "        loss = 0.\n",
        "\n",
        "        T = len(tr)\n",
        "        for t in range(T):\n",
        "            R = 0.\n",
        "            for i in range(t,T):\n",
        "                R += (gamma**(i - t))*tr[i][2]\n",
        "\n",
        "            loss += -alpha*R*Categorical(model(tr[t][0])).log_prob(tr[t][1])\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(time.time() - time_1)"
      ],
      "metadata": {
        "id": "uJUmT_Q-lpsf"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWai2UYGlqUi",
        "outputId": "ca447e88-f73f-4dbe-d22b-c0fbcff23997"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model loaded\n",
            "Neural wins per 1000 plays 0: errors per 1000 plays 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/container.py:204: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Neural wins per 1000 plays 32: errors per 1000 plays 3\n",
            "Neural wins per 1000 plays 33: errors per 1000 plays 1\n",
            "Neural wins per 1000 plays 44: errors per 1000 plays 3\n",
            "Neural wins per 1000 plays 32: errors per 1000 plays 3\n",
            "Neural wins per 1000 plays 36: errors per 1000 plays 1\n",
            "Neural wins per 1000 plays 35: errors per 1000 plays 3\n",
            "Neural wins per 1000 plays 32: errors per 1000 plays 2\n",
            "Neural wins per 1000 plays 42: errors per 1000 plays 5\n",
            "Neural wins per 1000 plays 30: errors per 1000 plays 0\n",
            "Neural wins per 1000 plays 50: errors per 1000 plays 7\n",
            "Neural wins per 1000 plays 42: errors per 1000 plays 5\n",
            "Neural wins per 1000 plays 36: errors per 1000 plays 2\n",
            "Neural wins per 1000 plays 39: errors per 1000 plays 4\n",
            "Neural wins per 1000 plays 44: errors per 1000 plays 4\n",
            "Neural wins per 1000 plays 346: errors per 1000 plays 7\n",
            "Neural wins per 1000 plays 371: errors per 1000 plays 3\n",
            "Neural wins per 1000 plays 645: errors per 1000 plays 5\n",
            "Neural wins per 1000 plays 595: errors per 1000 plays 9\n",
            "Neural wins per 1000 plays 273: errors per 1000 plays 4\n",
            "Neural wins per 1000 plays 246: errors per 1000 plays 0\n",
            "Neural wins per 1000 plays 287: errors per 1000 plays 2\n",
            "Neural wins per 1000 plays 215: errors per 1000 plays 0\n",
            "Neural wins per 1000 plays 236: errors per 1000 plays 8\n",
            "Neural wins per 1000 plays 361: errors per 1000 plays 1\n",
            "Neural wins per 1000 plays 310: errors per 1000 plays 4\n",
            "Neural wins per 1000 plays 68: errors per 1000 plays 0\n",
            "Neural wins per 1000 plays 90: errors per 1000 plays 0\n",
            "Neural wins per 1000 plays 126: errors per 1000 plays 1\n",
            "Neural wins per 1000 plays 112: errors per 1000 plays 6\n",
            "Neural wins per 1000 plays 115: errors per 1000 plays 2\n",
            "Neural wins per 1000 plays 99: errors per 1000 plays 3\n",
            "Neural wins per 1000 plays 103: errors per 1000 plays 5\n",
            "Neural wins per 1000 plays 122: errors per 1000 plays 5\n",
            "Neural wins per 1000 plays 219: errors per 1000 plays 3\n",
            "Neural wins per 1000 plays 299: errors per 1000 plays 2\n",
            "Neural wins per 1000 plays 150: errors per 1000 plays 0\n",
            "Neural wins per 1000 plays 209: errors per 1000 plays 5\n"
          ]
        }
      ]
    }
  ]
}